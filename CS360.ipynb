{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS360",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z-xiaojie/pythonProject/blob/master/CS360.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwwTgRxAark0",
        "colab_type": "text"
      },
      "source": [
        "Check the availability of CPU enabled computing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoBEbYILTMJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "size_of_problem = 400\n",
        "\n",
        "def computing(b):\n",
        "  for _ in range(1000000):\n",
        "      b += b\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('GPU:',torch.cuda.get_device_name(0))\n",
        "  print(\"GPU enabled, calcualting....\")\n",
        "  ### using GPU for parallel computation\n",
        "  start_time = time.time()\n",
        "  ### define problem\n",
        "  \"\"\"\n",
        "    .cuda() we are moving data from CPU to GPU and thus all the operatios will be executed on GPU\n",
        "  \"\"\"\n",
        "  b = torch.ones(size_of_problem,size_of_problem).cuda()\n",
        "  ### run computing\n",
        "  computing(b)\n",
        "  ### running time\n",
        "  elapsed_time_GPU = time.time() - start_time\n",
        "  print('GPU time = ',elapsed_time_GPU)\n",
        "else:\n",
        "  print(\"Can only use CPU.\")\n",
        "  \n",
        "### using CPU for parallel computation\n",
        "start_time = time.time()\n",
        "a = torch.ones(size_of_problem,size_of_problem)\n",
        "### run computing\n",
        "computing(a)\n",
        "### running time\n",
        "elapsed_time_CPU = time.time() - start_time\n",
        "print('CPU time = ',elapsed_time_CPU)\n",
        "\n",
        "print('Memory Usage:')\n",
        "print('Allocated:', torch.cuda.memory_allocated(0))\n",
        "print('Cached:   ', torch.cuda.memory_cached(0))\n",
        "\n",
        "print(\"speed up:\", elapsed_time_CPU / elapsed_time_GPU)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4cKMZiJcDzk",
        "colab_type": "text"
      },
      "source": [
        "Get the name of GPU."
      ]
    }
  ]
}